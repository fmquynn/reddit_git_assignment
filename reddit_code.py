# -*- coding: utf-8 -*-
"""reddit_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cW7Vvs8fCQcIvj_T6OyNxF97Bndtg0aJ

# Reddit API Assignment

This notebook downloads hot posts about climate, renewable energy, and sustainability from reddit, searches posts for key words and saves them to a CSV file.

**Author**: Frances Quynn

**Created**: November 3, 2025

**Purpose**: Collect reddit posts on climate, renewable energy, and substainability.

## Import Required Libraries

Import all necessary libraries for Reddit API access, CSV handling, and environment variable management.
"""

#%pip install praw python-dotenv

#%pip install pandas

# -*- coding: utf-8 -*-
import praw
import csv
import os
from dotenv import load_dotenv
import pandas as pd
print("Libraries imported successfully!")

"""## Load Environment Variables

Load Reddit API credentials from the environment file.

"""

from google.colab import drive
drive.mount('/content/drive')

# Load environment variables from .env file
# load_dotenv('reddit_api.env') # This line was commented out
from dotenv import dotenv_values
import os

# Define the path to your .env file in Google Drive
# IMPORTANT: Update this path to the actual location of your reddit_api.env file in your Google Drive
env_file_path = '/content/reddit.env'
#env_file_path = '/content/drive/MyDrive/Colab Notebooks/MSBA 212 - Social Media Analytics/Reddit_API_Assignment'


# Load environment variables from reddit_api.env file if it exists
if os.path.exists(env_file_path):
    config = dotenv_values(env_file_path)
    print(f"Environment variables loaded from {env_file_path}!")
else:
    config = {}
    print(f"Error: '{env_file_path}' not found. Environment variables not loaded.")
    print("Please ensure the 'reddit_api.env' file is in the specified Google Drive path.")

"""## Authenticate with Reddit API

Establish connection to Reddit using PRAW with the loaded credentials.

"""

# Authenticate with Reddit using environment variables
reddit = praw.Reddit(
    client_id=config.get('REDDIT_CLIENT_ID'),
    client_secret=config.get('REDDIT_CLIENT_SECRET'),
    username=config.get('REDDIT_USERNAME'),
    #password=config.get('REDDIT_PASSWORD'),
    user_agent=config.get('REDDIT_USER_AGENT')
)

print("Reddit API authenticated successfully!")
print(f"Connected as: {reddit.user.me()}")

"""## Task 1: Fetching hot posts

"""

# Define attributes to extract
def extract_post_data(post, search_query=None):
    return {
        "id": getattr(post, "id", None),
        "title": getattr(post, "title", None),
        "score": getattr(post, "score", None),
        "upvote_ratio": getattr(post, "upvote_ratio", None),
        "num_comments": getattr(post, "num_comments", None),
        "author": str(post.author) if post.author else None,
        "subreddit": getattr(post.subreddit, "display_name", None),
        "url": getattr(post, "url", None),
        "permalink": getattr(post, "permalink", None),
        "created_utc": getattr(post, "created_utc", None),
        "is_self": getattr(post, "is_self", None),
        "selftext": (post.selftext[:500] if post.selftext else None),
        "flair": getattr(post, "link_flair_text", None),
        "domain": getattr(post, "domain", None),
        "search_query": search_query  # None for hot posts, filled in for keyword search
    }

def fetch_hot_posts(subreddits, limit=50):
    all_posts = []
    for sub in subreddits:
        try:
            subreddit = reddit.subreddit(sub.replace("r/", ""))
            hot_posts = subreddit.hot(limit=limit)
            posts_data = [extract_post_data(post) for post in hot_posts]
            all_posts.extend(posts_data)
            print(f"Collected {len(posts_data)} posts from r/{sub}.")
        except Exception as e:
            print(f"Error fetching from {sub}: {e}")
    return pd.DataFrame(all_posts)

# Run the function
subreddits = ["climate", "renewableenergy", "Sustainability"]
df_hot = fetch_hot_posts(subreddits)

"""## Task 2: Keyword-Based Search"""

def search_posts(query, subreddits, limit=50):
    results = []
    for sub in subreddits:
        try:
            subreddit = reddit.subreddit(sub.replace("r/", ""))
            search_results = subreddit.search(query, limit=limit)
            for post in search_results:
                post_data = {
                    "id": getattr(post, "id", None),
                    "title": getattr(post, "title", None),
                    "score": getattr(post, "score", None),
                    "upvote_ratio": getattr(post, "upvote_ratio", None),
                    "num_comments": getattr(post, "num_comments", None),
                    "author": str(post.author) if post.author else None,
                    "subreddit": getattr(post.subreddit, "display_name", None),
                    "url": getattr(post, "url", None),
                    "permalink": getattr(post, "permalink", None),
                    "created_utc": getattr(post, "created_utc", None),
                    "is_self": getattr(post, "is_self", None),
                    "selftext": post.selftext[:500] if post.selftext else None,
                    "flair": getattr(post, "link_flair_text", None),
                    "domain": getattr(post, "domain", None),
                    "search_query": query
                }
                results.append(post_data)
            print(f"Collected {len(results)} posts from r/{sub} using keyword '{query}'.")
        except Exception as e:
            print(f"Error searching r/{sub} for '{query}': {e}")
    return pd.DataFrame(results)


df_search = search_posts("weather", subreddits)

"""## Task 3: Data Export to CSV"""

def process_and_export_to_csv(posts, filename="reddit_data.csv"):
    # Convert to DataFrame
    df = pd.DataFrame(posts)

    # Truncate selftext to 500 characters
    if "selftext" in df.columns:
        df["selftext"] = df["selftext"].apply(lambda x: x[:500] if isinstance(x, str) else None)

    # Deduplicate by post ID or permalink
    if "id" in df.columns:
        df.drop_duplicates(subset="id", inplace=True)
    elif "permalink" in df.columns:
        df.drop_duplicates(subset="permalink", inplace=True)

    # Ensure all required columns exist
    required_columns = [
        "title", "score", "upvote_ratio", "num_comments", "author", "subreddit",
        "url", "permalink", "created_utc", "is_self", "selftext", "flair", "domain", "search_query"
    ]
    for col in required_columns:
        if col not in df.columns:
            df[col] = None  # Fill missing columns with nulls

    # Reorder columns
    df = df[required_columns]

    # Export to CSV without index
    df.to_csv(filename, index=False)
    print(f"Saved {len(df)} posts to {filename}.")

# Combine both
df_combined = pd.concat([df_hot, df_search], ignore_index=True)

# Optional: Deduplicate by post ID or permalink
df_combined.drop_duplicates(subset=["id", "permalink"], inplace=True)

# Export
process_and_export_to_csv(df_combined)

"""## Creating a README.md file"""

content = """
# Reddit API Assignment
### Frances Quynn

## Overview
This notebook downloads popular posts about climate, renewable energy,
and sustainability from Reddit, searches posts with the key word "weather"
and then saves them to a CSV file. The Python script reads in API credentials
(Client ID, Client Secret, User-Agent) from the reddit_api_template.env file to
connect to the API.

## How to Run
1. Prerequisites
 - Python 3.8 or higher
 - Reddit API credentials (client ID, client secret, user agent)
2. Installation
 - pip install -r requirements.txt
3. Configuration: Create a .env file (reddit.env) and put it in the
   same folder as Python script with the following format:
 - REDDIT_CLIENT_ID=your_client_id_here
 - REDDIT_CLIENT_SECRET=your_client_secret_here
 - REDDIT_USER_AGENT=your_user_agent_here
4. Execution: To execute, enter the command "python reddit_code.py" in your command line.
   If using Google Colab, ensure your Drive is mounted and the .env path is correctly set in your notebook.

## Output
The output file reddit_data.csv includes the following columns:
- "title" of the post
- "score": (# upvotes - # downvotes)
- "upvote_ratio": Ratio of upvotes to total votes
- "num_comments" - number of comments per each post.
- "author": username of the post's author
- "subreddit" - which of the 3 subreddits the post belongs to: climate, RenewableEnergy, or sustainability
- "url" - The URL of the post
- "permalink" - The permanent URL to the Reddit post itself
- "created_utc" - the Unix timestamp of post creation.
- "is_self" - True if the post is text-only, otherwise False.
- "selftext" - The body of the post (truncated to 500 chars)
- "flair" - The category tag assigned to the post (in this case, politics or science)
- "domain" - the domain of the linked content.
- "search_query" - search term that marks posts that contain the keyword "weather".

Within the file, we see there are a lot of posts on the climate crisis and political
posts about the Trump administration cancelling federal funding for renewable energy projects.
"""
with open("README.md", "w") as f:
    f.write(content)